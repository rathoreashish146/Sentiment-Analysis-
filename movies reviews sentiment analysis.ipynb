{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39082d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef724a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we take data from KAGGLE and data set name is IMDB 50k Movies dataset\n",
    "df=pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bf22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## just checking how data look like\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bddb6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## one review\n",
    "# df['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fbb02",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59c1a4",
   "metadata": {},
   "source": [
    "1. Sample 10000 rows\n",
    "2. Remove html tags\n",
    "3. remove special character\n",
    "4. Converting every thing to lower case\n",
    "5. Removing stop words\n",
    "6. stemming [play,playing,played] => [play,play,play]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b8d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 1 taking 10k data points only\n",
    "df=df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e258aab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##just checking shape of data fram\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a5f0478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 33878 to 569\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     1000 non-null   object\n",
      " 1   sentiment  1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "## just checking missing points etc.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5404cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace sentiment word by 1 and 0 for +ve and -ve reviews\n",
    "df['sentiment'].replace({'positive':1,'negative':0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73eec55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## just checking how data look like\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a1df1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Legendary pop star Steve Alaimo (\"Don\\'t Let the Sun Catch You Crying\") stars as an unlikable stock car racer whose career has hit the skids (ha ha) because he constantly crashes his car (or as he laments, \"I\\'m tired of being run down by every grease monkey that gets behind the wheel\"). He falls into a bad crowd of humorously inept Nazi bikers, improbably named Jeeter, Banjo, Fats, and, er, Linda. Fats is the most likable of the bunch; he took a surfboard to the back of the skull and now only communicates through grunts, sort of a Harley Davidson Leatherface.Anyway, Steve is fooled by the cops into catching the dastardly crooks in the middle of one of their bank robberies. The gang only robs banks for \"kicks, man\". I guess they give the money to charity. Steve fails constantly, the bikers get greasier, and the whole thing never comes off as daring because it\\'s so dull. This movie looks like it was filmed through a grease-soaked paper towel. Not since \"Necromantic\" has a movie so trampled my soul.The guy who played Fats went on to direct \"Deranged\", the Ed Gein biopic starring Robert Blossom.Anyway, in summary: Wild Rebels: Hilarious on MST3K, dreadful everywhere else.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 2 to Remove all html tags using re library \n",
    "## checking our function\n",
    "import re\n",
    "clean = re.compile('<.*?>')\n",
    "re.sub(clean,'',df.iloc[2].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce06fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 2 to Remove all html tags using re library\n",
    "## creating a function to clean html tags\n",
    "def clean_html(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean,'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "015df8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd11e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## one review\n",
    "# df.iloc[2].review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b05c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 4 Converting every thing to lower case\n",
    "def convert_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f6e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(convert_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a2c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## one review\n",
    "# df.iloc[2].review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3bd988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 3 to remove all special character\n",
    "def remove_special(text):\n",
    "    x=''\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            x=x+i\n",
    "        else:\n",
    "            x=x+' '\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3b43a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75cded4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## one review\n",
    "# df.iloc[2].review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36facc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 5 to remove stop words\n",
    "import nltk ## most used library in NLP\n",
    "## Natural language tool kit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22a10dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39b19360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3159793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    x=[]\n",
    "    for i in text.split():\n",
    "        if i not in stopwords.words('english'):\n",
    "            x.append(i)\n",
    "    y=x[:]\n",
    "    x.clear()\n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "190e24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "831abbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33878</th>\n",
       "      <td>[dick, tracy, one, family, favorites, actors, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>[heard, driving, lessons, itv, adverts, honest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17013</th>\n",
       "      <td>[legendary, pop, star, steve, alaimo, let, sun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10271</th>\n",
       "      <td>[one, films, whatever, reason, clicked, everyt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13181</th>\n",
       "      <td>[enjoyable, parts, film, clips, original, movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23496</th>\n",
       "      <td>[story, little, slow, little, stupid, greta, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41294</th>\n",
       "      <td>[reading, many, glowing, reports, serve, days,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41704</th>\n",
       "      <td>[one, worst, sandra, bullock, movie, since, sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38879</th>\n",
       "      <td>[minor, spoilersalison, parker, cristina, rain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>[watched, movie, tv, last, night, hoping, real...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "33878  [dick, tracy, one, family, favorites, actors, ...          1\n",
       "722    [heard, driving, lessons, itv, adverts, honest...          1\n",
       "17013  [legendary, pop, star, steve, alaimo, let, sun...          0\n",
       "10271  [one, films, whatever, reason, clicked, everyt...          1\n",
       "13181  [enjoyable, parts, film, clips, original, movi...          0\n",
       "...                                                  ...        ...\n",
       "23496  [story, little, slow, little, stupid, greta, g...          0\n",
       "41294  [reading, many, glowing, reports, serve, days,...          0\n",
       "41704  [one, worst, sandra, bullock, movie, since, sp...          0\n",
       "38879  [minor, spoilersalison, parker, cristina, rain...          1\n",
       "569    [watched, movie, tv, last, night, hoping, real...          0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## one review\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f537e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## last step stemming \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fddab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "def stem_words(text):\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    z=y[:]\n",
    "    y.clear()\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12ad6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54482f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## join back\n",
    "def join_back(list_input):\n",
    "    return \" \".join(list_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "162e3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(join_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e15595ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,0:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9cd89bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8cffd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we are creating a 2d array in the form of features and lebals\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a46e3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(df['review']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ae1eb494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 850)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "08f539f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5268a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ae241225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "409a72f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data spliting into 2 part \n",
    "##one for training called training set \n",
    "## other one for testing called testing set (unseen data) \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e4bce684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 850)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "986a53cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 850)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bb555c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e48bc64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "871e15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ab8f0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=GaussianNB()\n",
    "clf2=MultinomialNB()\n",
    "clf3=BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "44ca19e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X_train,y_train)\n",
    "clf2.fit(X_train,y_train)\n",
    "clf3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2c748073",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=clf1.predict(X_test)\n",
    "y_pred2=clf2.predict(X_test)\n",
    "y_pred3=clf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1c1466e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "aa784b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB 0.78\n",
      "MultinomialNB 0.86\n",
      "BernoulliNB 0.845\n"
     ]
    }
   ],
   "source": [
    "print(\"GaussianNB\",accuracy_score(y_test,y_pred1))\n",
    "print(\"MultinomialNB\",accuracy_score(y_test,y_pred2))\n",
    "print(\"BernoulliNB\",accuracy_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e93587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
